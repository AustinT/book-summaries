# Superintelligence
## Nick Bostrom

A very dense book of speculation about the nature of superintelligence: what forms it might take, what it might do, and under what conditions it might be developed.
The apparent goal of the book is to convince the reader that superintelligence poses some existential risk to humanity, and that we should guide our quest towards superintelligence in a way that minimizes this risk.

I don't think I can give a fair summary of the many ideas that the book presents, but here are a few ones I found interesting:
- AI might be reached by whole brain emulation instead of through completely synthetic means. This avenue is much more predictable, and we have a rough idea of how we might accomplish it in theory (unlike synthetic AI). But it still has a lot of risks (humans do bad/unpredictable things too), and there are many potential ethical questions about creating artificial [presumably conscious] humans.
- Risks of AI depend a lot on the _dynamics_ of the intelligence development: if it is very slow, there will be lots of time to prepare and map out the future of AI while it is advancing. If the intelligence is super-quickly developed, then it is possible that the first AI made will become a single superintelligence that could in theory impose whatever conditions it would like on the world (forming a _singleton_).
- A lot of AI discussion focuses on the goals of a superintelligence: will it be benevolent and help humanity, or be determined to eradicate us?
 This makes it seem like _end goal specification_ is the most important problem in AI.
 However, Bostrom raises the interesting point that world domination might emerge as an _instrumental goal_ of AIs with a wide range of goals.
 For example, appropriating the world's resources allows for more hardware to be built to better compute how to help humanity, so even an AI with a benevolent goal like "help humanity" might end up doing harmful things in pursuit of its goal.
 Another example is that eliminating humanity would reduce the risk of an AI being shut off (and thereby unable to accomplish its goal),
 meaning that an AI might see eliminating humanity as a means to ensure the completion of a wide variety of goals.
 Therefore aligning an AI isn't just a matter of specifying the right end goal, but also making sure it acts properly in achieving that end goal.
- Controlling an AI is very difficult.
 You can't just "box it" somewhere safe, since any interaction is a potential way for it to escape confinement. If it is completely 100% boxed then the AI has no point; we wouldn't be able to observe or study it, or use it to solve any problems.
 Some more clever ways of control have been theorized, but none are perfect.
- The problem of what values to give AIs is incredibly difficult, especially when you consider that people don't yet agree on what values _people_ should be given.

Overall, reading this book has made me more aware of the many difficulties and problems that could come out of the development of AI, and that a cautionary mindset is incredibly important.
